# Generate the confusion matrix showing counts.
rattle::errorMatrix(na.omit(crs$dataset[crs$test, c(crs$input, crs$target)])$failure, crs$pr, count=TRUE)
# Generate the confusion matrix showing proportions.
(per <- rattle::errorMatrix(na.omit(crs$dataset[crs$test, c(crs$input, crs$target)])$failure, crs$pr))
# Calculate the overall error percentage.
cat(100-sum(diag(per), na.rm=TRUE))
# Calculate the averaged class error percentage.
cat(mean(per[,"Error"], na.rm=TRUE))
#=======================================================================
# Rattle timestamp: 2019-03-04 00:25:37 x86_64-w64-mingw32
# Evaluate model performance on the testing dataset.
# ROC Curve: requires the ROCR package.
library(ROCR)
# ROC Curve: requires the ggplot2 package.
library(ggplot2, quietly=TRUE)
# Generate an ROC Curve for the ksvm model on normalized.csv [test].
crs$pr <- kernlab::predict(crs$ksvm, newdata=na.omit(crs$dataset[crs$test, c(crs$input, crs$target)]),
type    = "probabilities")[,2]
# Remove observations with missing target.
no.miss   <- na.omit(na.omit(crs$dataset[crs$test, c(crs$input, crs$target)])$failure)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL
if (length(miss.list))
{
pred <- prediction(crs$pr[-miss.list], no.miss)
} else
{
pred <- prediction(crs$pr, no.miss)
}
pe <- performance(pred, "tpr", "fpr")
au <- performance(pred, "auc")@y.values[[1]]
pd <- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))
p <- ggplot(pd, aes(x=fpr, y=tpr))
p <- p + geom_line(colour="red")
p <- p + xlab("False Positive Rate") + ylab("True Positive Rate")
p <- p + ggtitle("ROC Curve SVM normalized.csv [test] failure")
p <- p + theme(plot.title=element_text(size=10))
p <- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour="grey")
p <- p + annotate("text", x=0.50, y=0.00, hjust=0, vjust=0, size=5,
label=paste("AUC =", round(au, 2)))
print(p)
# Calculate the area under the curve for the plot.
# Remove observations with missing target.
no.miss   <- na.omit(na.omit(crs$dataset[crs$test, c(crs$input, crs$target)])$failure)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL
if (length(miss.list))
{
pred <- prediction(crs$pr[-miss.list], no.miss)
} else
{
pred <- prediction(crs$pr, no.miss)
}
performance(pred, "auc")
#=======================================================================
# Rattle timestamp: 2019-03-04 00:25:42 x86_64-w64-mingw32
# Evaluate model performance on the testing dataset.
# Precision/Recall Plot: requires the ROCR package
library(ROCR)
# Generate a Precision/Recall Plot for the ksvm model on normalized.csv [test].
crs$pr <- kernlab::predict(crs$ksvm, newdata=na.omit(crs$dataset[crs$test, c(crs$input, crs$target)]),
type    = "probabilities")[,2]
# Remove observations with missing target.
no.miss   <- na.omit(na.omit(crs$dataset[crs$test, c(crs$input, crs$target)])$failure)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL
if (length(miss.list))
{
pred <- prediction(crs$pr[-miss.list], no.miss)
} else
{
pred <- prediction(crs$pr, no.miss)
}
ROCR::plot(performance(pred, "prec", "rec"), col="#CC0000FF", lty=1, add=FALSE)
# Generate a Precision/Recall Plot for the ksvm model on normalized.csv [train].
crs$pr <- kernlab::predict(crs$ksvm, newdata=na.omit(crs$dataset[crs$test, c(crs$input, crs$target)]),
type    = "probabilities")[,2]
# In ROCR (1.0-3) plot does not obey the add command.
# Calling the function directly (.plot.performance) does work.
.plot.performance(performance(prediction(crs$pr, na.omit(crs$dataset[crs$test, c(crs$input, crs$target)])$failure),"prec", "rec"), col="#00CCCCFF", lty=2, add=TRUE)
# Add a legend to the plot.
legend("bottomleft", c("Test","Train"), col=rainbow(2, 1, .8), lty=1:2, title="ksvm", inset=c(0.05, 0.05))
# Add decorations to the plot.
title(main="Precision/Recall Plot  normalized.csv ",
sub=paste("Rattle", format(Sys.time(), "%Y-%b-%d %H:%M:%S"), Sys.info()["user"]))
grid()
#=======================================================================
# Rattle timestamp: 2019-03-04 00:25:46 x86_64-w64-mingw32
# Evaluate model performance on the testing dataset.
# Sensitivity/Specificity Plot: requires the ROCR package
library(ROCR)
# Generate Sensitivity/Specificity Plot for ksvm model on normalized.csv [test].
crs$pr <- kernlab::predict(crs$ksvm, newdata=na.omit(crs$dataset[crs$test, c(crs$input, crs$target)]),
type    = "probabilities")[,2]
# Remove observations with missing target.
no.miss   <- na.omit(na.omit(crs$dataset[crs$test, c(crs$input, crs$target)])$failure)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL
if (length(miss.list))
{
pred <- prediction(crs$pr[-miss.list], no.miss)
} else
{
pred <- prediction(crs$pr, no.miss)
}
ROCR::plot(performance(pred, "sens", "spec"), col="#CC0000FF", lty=1, add=FALSE)
# Generate a Lift Chart for the ksvm model on normalized.csv [train].
crs$pr <- kernlab::predict(crs$ksvm, newdata=na.omit(crs$dataset[crs$test, c(crs$input, crs$target)]),
type    = "probabilities")[,2]
#In ROCR (1.0-3) plot does not obey the add command.
# Calling the function directly (.plot.performance) does work.
.plot.performance(performance(prediction(crs$pr, na.omit(crs$dataset[crs$test, c(crs$input, crs$target)])$failure),"sens", "spec"), col="#00CCCCFF", lty=2, add=TRUE)
# Add a legend to the plot.
legend("bottomleft", c("Test","Train"), col=rainbow(2, 1, .8), lty=1:2, title="ksvm", inset=c(0.05, 0.05))
# Add decorations to the plot.
title(main="Sensitivity/Specificity (tpr/tnr)  normalized.csv ",
sub=paste("Rattle", format(Sys.time(), "%Y-%b-%d %H:%M:%S"), Sys.info()["user"]))
grid()
#=======================================================================
# Rattle timestamp: 2019-03-04 00:25:49 x86_64-w64-mingw32
# Score the testing dataset.
#=======================================================================
# Rattle timestamp: 2019-03-04 00:26:17 x86_64-w64-mingw32
# Regression model
# Build a Regression model.
crs$glm <- glm(failure ~ .,
data=crs$dataset[crs$train, c(crs$input, crs$target)],
family=binomial(link="logit"))
# Generate a textual view of the Linear model.
print(summary(crs$glm))
cat(sprintf("Log likelihood: %.3f (%d df)\n",
logLik(crs$glm)[1],
attr(logLik(crs$glm), "df")))
cat(sprintf("Null/Residual deviance difference: %.3f (%d df)\n",
crs$glm$null.deviance-crs$glm$deviance,
crs$glm$df.null-crs$glm$df.residual))
cat(sprintf("Chi-square p-value: %.8f\n",
dchisq(crs$glm$null.deviance-crs$glm$deviance,
crs$glm$df.null-crs$glm$df.residual)))
cat(sprintf("Pseudo R-Square (optimistic): %.8f\n",
cor(crs$glm$y, crs$glm$fitted.values)))
cat('\n==== ANOVA ====\n\n')
print(anova(crs$glm, test="Chisq"))
cat("\n")
# Time taken: 6.59 secs
#=======================================================================
# Rattle timestamp: 2019-03-04 00:26:36 x86_64-w64-mingw32
# Evaluate model performance on the training dataset.
# Generate an Error Matrix for the Linear model.
# Obtain the response from the Linear model.
crs$pr <- as.vector(ifelse(predict(crs$glm,
type    = "response",
newdata = crs$dataset[crs$train, c(crs$input, crs$target)]) > 0.5, "1", "0"))
# Generate the confusion matrix showing counts.
rattle::errorMatrix(crs$dataset[crs$train, c(crs$input, crs$target)]$failure, crs$pr, count=TRUE)
# Generate the confusion matrix showing proportions.
(per <- rattle::errorMatrix(crs$dataset[crs$train, c(crs$input, crs$target)]$failure, crs$pr))
# Calculate the overall error percentage.
cat(100-sum(diag(per), na.rm=TRUE))
# Calculate the averaged class error percentage.
cat(mean(per[,"Error"], na.rm=TRUE))
#=======================================================================
# Rattle timestamp: 2019-03-04 00:26:40 x86_64-w64-mingw32
# Evaluate model performance on the validation dataset.
# Generate an Error Matrix for the Linear model.
# Obtain the response from the Linear model.
crs$pr <- as.vector(ifelse(predict(crs$glm,
type    = "response",
newdata = crs$dataset[crs$validate, c(crs$input, crs$target)]) > 0.5, "1", "0"))
# Generate the confusion matrix showing counts.
rattle::errorMatrix(crs$dataset[crs$validate, c(crs$input, crs$target)]$failure, crs$pr, count=TRUE)
# Generate the confusion matrix showing proportions.
(per <- rattle::errorMatrix(crs$dataset[crs$validate, c(crs$input, crs$target)]$failure, crs$pr))
# Calculate the overall error percentage.
cat(100-sum(diag(per), na.rm=TRUE))
# Calculate the averaged class error percentage.
cat(mean(per[,"Error"], na.rm=TRUE))
#=======================================================================
# Rattle timestamp: 2019-03-04 00:26:42 x86_64-w64-mingw32
# Evaluate model performance on the testing dataset.
# Generate an Error Matrix for the Linear model.
# Obtain the response from the Linear model.
crs$pr <- as.vector(ifelse(predict(crs$glm,
type    = "response",
newdata = crs$dataset[crs$test, c(crs$input, crs$target)]) > 0.5, "1", "0"))
# Generate the confusion matrix showing counts.
rattle::errorMatrix(crs$dataset[crs$test, c(crs$input, crs$target)]$failure, crs$pr, count=TRUE)
# Generate the confusion matrix showing proportions.
(per <- rattle::errorMatrix(crs$dataset[crs$test, c(crs$input, crs$target)]$failure, crs$pr))
# Calculate the overall error percentage.
cat(100-sum(diag(per), na.rm=TRUE))
# Calculate the averaged class error percentage.
cat(mean(per[,"Error"], na.rm=TRUE))
#=======================================================================
# Rattle timestamp: 2019-03-04 00:26:46 x86_64-w64-mingw32
# Evaluate model performance on the testing dataset.
# ROC Curve: requires the ROCR package.
library(ROCR)
# ROC Curve: requires the ggplot2 package.
library(ggplot2, quietly=TRUE)
# Generate an ROC Curve for the glm model on normalized.csv [test].
crs$pr <- predict(crs$glm,
type    = "response",
newdata = crs$dataset[crs$test, c(crs$input, crs$target)])
# Remove observations with missing target.
no.miss   <- na.omit(crs$dataset[crs$test, c(crs$input, crs$target)]$failure)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL
if (length(miss.list))
{
pred <- prediction(crs$pr[-miss.list], no.miss)
} else
{
pred <- prediction(crs$pr, no.miss)
}
pe <- performance(pred, "tpr", "fpr")
au <- performance(pred, "auc")@y.values[[1]]
pd <- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))
p <- ggplot(pd, aes(x=fpr, y=tpr))
p <- p + geom_line(colour="red")
p <- p + xlab("False Positive Rate") + ylab("True Positive Rate")
p <- p + ggtitle("ROC Curve Linear normalized.csv [test] failure")
p <- p + theme(plot.title=element_text(size=10))
p <- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour="grey")
p <- p + annotate("text", x=0.50, y=0.00, hjust=0, vjust=0, size=5,
label=paste("AUC =", round(au, 2)))
print(p)
# Calculate the area under the curve for the plot.
# Remove observations with missing target.
no.miss   <- na.omit(crs$dataset[crs$test, c(crs$input, crs$target)]$failure)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL
if (length(miss.list))
{
pred <- prediction(crs$pr[-miss.list], no.miss)
} else
{
pred <- prediction(crs$pr, no.miss)
}
performance(pred, "auc")
#=======================================================================
# Rattle timestamp: 2019-03-04 00:26:49 x86_64-w64-mingw32
# Evaluate model performance on the testing dataset.
# Precision/Recall Plot: requires the ROCR package
library(ROCR)
# Generate a Precision/Recall Plot for the glm model on normalized.csv [test].
crs$pr <- predict(crs$glm,
type    = "response",
newdata = crs$dataset[crs$test, c(crs$input, crs$target)])
# Remove observations with missing target.
no.miss   <- na.omit(crs$dataset[crs$test, c(crs$input, crs$target)]$failure)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL
if (length(miss.list))
{
pred <- prediction(crs$pr[-miss.list], no.miss)
} else
{
pred <- prediction(crs$pr, no.miss)
}
ROCR::plot(performance(pred, "prec", "rec"), col="#CC0000FF", lty=1, add=FALSE)
# Generate a Precision/Recall Plot for the glm model on normalized.csv [train].
crs$pr <- predict(crs$glm,
type    = "response",
newdata = crs$dataset[crs$test, c(crs$input, crs$target)])
# In ROCR (1.0-3) plot does not obey the add command.
# Calling the function directly (.plot.performance) does work.
.plot.performance(performance(prediction(crs$pr, crs$dataset[crs$test, c(crs$input, crs$target)]$failure),"prec", "rec"), col="#00CCCCFF", lty=2, add=TRUE)
# Add a legend to the plot.
legend("bottomleft", c("Test","Train"), col=rainbow(2, 1, .8), lty=1:2, title="glm", inset=c(0.05, 0.05))
# Add decorations to the plot.
title(main="Precision/Recall Plot  normalized.csv ",
sub=paste("Rattle", format(Sys.time(), "%Y-%b-%d %H:%M:%S"), Sys.info()["user"]))
grid()
#=======================================================================
# Rattle timestamp: 2019-03-04 00:27:10 x86_64-w64-mingw32
# Evaluate model performance on the testing dataset.
# Sensitivity/Specificity Plot: requires the ROCR package
library(ROCR)
# Generate Sensitivity/Specificity Plot for glm model on normalized.csv [test].
crs$pr <- predict(crs$glm,
type    = "response",
newdata = crs$dataset[crs$test, c(crs$input, crs$target)])
# Remove observations with missing target.
no.miss   <- na.omit(crs$dataset[crs$test, c(crs$input, crs$target)]$failure)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL
if (length(miss.list))
{
pred <- prediction(crs$pr[-miss.list], no.miss)
} else
{
pred <- prediction(crs$pr, no.miss)
}
ROCR::plot(performance(pred, "sens", "spec"), col="#CC0000FF", lty=1, add=FALSE)
# Generate a Lift Chart for the glm model on normalized.csv [train].
crs$pr <- predict(crs$glm,
type    = "response",
newdata = crs$dataset[crs$test, c(crs$input, crs$target)])
#In ROCR (1.0-3) plot does not obey the add command.
# Calling the function directly (.plot.performance) does work.
.plot.performance(performance(prediction(crs$pr, crs$dataset[crs$test, c(crs$input, crs$target)]$failure),"sens", "spec"), col="#00CCCCFF", lty=2, add=TRUE)
# Add a legend to the plot.
legend("bottomleft", c("Test","Train"), col=rainbow(2, 1, .8), lty=1:2, title="glm", inset=c(0.05, 0.05))
# Add decorations to the plot.
title(main="Sensitivity/Specificity (tpr/tnr)  normalized.csv ",
sub=paste("Rattle", format(Sys.time(), "%Y-%b-%d %H:%M:%S"), Sys.info()["user"]))
grid()
#=======================================================================
# Rattle timestamp: 2019-03-04 00:27:13 x86_64-w64-mingw32
# Score the testing dataset.
#=======================================================================
# Rattle timestamp: 2019-03-04 00:27:37 x86_64-w64-mingw32
# Neural Network
# Build a neural network model using the nnet package.
library(nnet, quietly=TRUE)
# Build the NNet model.
set.seed(199)
crs$nnet <- nnet(as.factor(failure) ~ .,
data=crs$dataset[crs$train,c(crs$input, crs$target)],
size=10, skip=TRUE, MaxNWts=10000, trace=FALSE, maxit=100)
# Print the results of the modelling.
cat(sprintf("A %s network with %d weights.\n",
paste(crs$nnet$n, collapse="-"),
length(crs$nnet$wts)))
cat(sprintf("Inputs: %s.\n",
paste(crs$nnet$coefnames, collapse=", ")))
cat(sprintf("Output: %s.\n",
names(attr(crs$nnet$terms, "dataClasses"))[1]))
cat(sprintf("Sum of Squares Residuals: %.4f.\n",
sum(residuals(crs$nnet) ^ 2)))
cat("\n")
print(summary(crs$nnet))
cat('\n')
# Time taken: 5.00 secs
#=======================================================================
# Rattle timestamp: 2019-03-04 00:28:04 x86_64-w64-mingw32
# Evaluate model performance on the training dataset.
# Generate an Error Matrix for the Neural Net model.
# Obtain the response from the Neural Net model.
crs$pr <- predict(crs$nnet, newdata=crs$dataset[crs$train, c(crs$input, crs$target)], type="class")
# Generate the confusion matrix showing counts.
rattle::errorMatrix(crs$dataset[crs$train, c(crs$input, crs$target)]$failure, crs$pr, count=TRUE)
# Generate the confusion matrix showing proportions.
(per <- rattle::errorMatrix(crs$dataset[crs$train, c(crs$input, crs$target)]$failure, crs$pr))
# Calculate the overall error percentage.
cat(100-sum(diag(per), na.rm=TRUE))
# Calculate the averaged class error percentage.
cat(mean(per[,"Error"], na.rm=TRUE))
#=======================================================================
# Rattle timestamp: 2019-03-04 00:28:09 x86_64-w64-mingw32
# Evaluate model performance on the validation dataset.
# Generate an Error Matrix for the Neural Net model.
# Obtain the response from the Neural Net model.
crs$pr <- predict(crs$nnet, newdata=crs$dataset[crs$validate, c(crs$input, crs$target)], type="class")
# Generate the confusion matrix showing counts.
rattle::errorMatrix(crs$dataset[crs$validate, c(crs$input, crs$target)]$failure, crs$pr, count=TRUE)
# Generate the confusion matrix showing proportions.
(per <- rattle::errorMatrix(crs$dataset[crs$validate, c(crs$input, crs$target)]$failure, crs$pr))
# Calculate the overall error percentage.
cat(100-sum(diag(per), na.rm=TRUE))
# Calculate the averaged class error percentage.
cat(mean(per[,"Error"], na.rm=TRUE))
#=======================================================================
# Rattle timestamp: 2019-03-04 00:28:11 x86_64-w64-mingw32
# Evaluate model performance on the testing dataset.
# Generate an Error Matrix for the Neural Net model.
# Obtain the response from the Neural Net model.
crs$pr <- predict(crs$nnet, newdata=crs$dataset[crs$test, c(crs$input, crs$target)], type="class")
# Generate the confusion matrix showing counts.
rattle::errorMatrix(crs$dataset[crs$test, c(crs$input, crs$target)]$failure, crs$pr, count=TRUE)
# Generate the confusion matrix showing proportions.
(per <- rattle::errorMatrix(crs$dataset[crs$test, c(crs$input, crs$target)]$failure, crs$pr))
# Calculate the overall error percentage.
cat(100-sum(diag(per), na.rm=TRUE))
# Calculate the averaged class error percentage.
cat(mean(per[,"Error"], na.rm=TRUE))
#=======================================================================
# Rattle timestamp: 2019-03-04 00:28:15 x86_64-w64-mingw32
# Evaluate model performance on the testing dataset.
# ROC Curve: requires the ROCR package.
library(ROCR)
# ROC Curve: requires the ggplot2 package.
library(ggplot2, quietly=TRUE)
# Generate an ROC Curve for the nnet model on normalized.csv [test].
crs$pr <- predict(crs$nnet, newdata=crs$dataset[crs$test, c(crs$input, crs$target)])
# Remove observations with missing target.
no.miss   <- na.omit(crs$dataset[crs$test, c(crs$input, crs$target)]$failure)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL
if (length(miss.list))
{
pred <- prediction(crs$pr[-miss.list], no.miss)
} else
{
pred <- prediction(crs$pr, no.miss)
}
pe <- performance(pred, "tpr", "fpr")
au <- performance(pred, "auc")@y.values[[1]]
pd <- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))
p <- ggplot(pd, aes(x=fpr, y=tpr))
p <- p + geom_line(colour="red")
p <- p + xlab("False Positive Rate") + ylab("True Positive Rate")
p <- p + ggtitle("ROC Curve Neural Net normalized.csv [test] failure")
p <- p + theme(plot.title=element_text(size=10))
p <- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour="grey")
p <- p + annotate("text", x=0.50, y=0.00, hjust=0, vjust=0, size=5,
label=paste("AUC =", round(au, 2)))
print(p)
# Calculate the area under the curve for the plot.
# Remove observations with missing target.
no.miss   <- na.omit(crs$dataset[crs$test, c(crs$input, crs$target)]$failure)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL
if (length(miss.list))
{
pred <- prediction(crs$pr[-miss.list], no.miss)
} else
{
pred <- prediction(crs$pr, no.miss)
}
performance(pred, "auc")
#=======================================================================
# Rattle timestamp: 2019-03-04 00:28:19 x86_64-w64-mingw32
# Evaluate model performance on the testing dataset.
# Precision/Recall Plot: requires the ROCR package
library(ROCR)
# Generate a Precision/Recall Plot for the nnet model on normalized.csv [test].
crs$pr <- predict(crs$nnet, newdata=crs$dataset[crs$test, c(crs$input, crs$target)])
# Remove observations with missing target.
no.miss   <- na.omit(crs$dataset[crs$test, c(crs$input, crs$target)]$failure)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL
if (length(miss.list))
{
pred <- prediction(crs$pr[-miss.list], no.miss)
} else
{
pred <- prediction(crs$pr, no.miss)
}
ROCR::plot(performance(pred, "prec", "rec"), col="#CC0000FF", lty=1, add=FALSE)
# Generate a Precision/Recall Plot for the nnet model on normalized.csv [train].
crs$pr <- predict(crs$nnet, newdata=crs$dataset[crs$test, c(crs$input, crs$target)])
# In ROCR (1.0-3) plot does not obey the add command.
# Calling the function directly (.plot.performance) does work.
.plot.performance(performance(prediction(crs$pr, crs$dataset[crs$test, c(crs$input, crs$target)]$failure),"prec", "rec"), col="#00CCCCFF", lty=2, add=TRUE)
# Add a legend to the plot.
legend("bottomleft", c("Test","Train"), col=rainbow(2, 1, .8), lty=1:2, title="nnet", inset=c(0.05, 0.05))
# Add decorations to the plot.
title(main="Precision/Recall Plot  normalized.csv ",
sub=paste("Rattle", format(Sys.time(), "%Y-%b-%d %H:%M:%S"), Sys.info()["user"]))
grid()
#=======================================================================
# Rattle timestamp: 2019-03-04 00:28:22 x86_64-w64-mingw32
# Evaluate model performance on the testing dataset.
# Sensitivity/Specificity Plot: requires the ROCR package
library(ROCR)
# Generate Sensitivity/Specificity Plot for nnet model on normalized.csv [test].
crs$pr <- predict(crs$nnet, newdata=crs$dataset[crs$test, c(crs$input, crs$target)])
# Remove observations with missing target.
no.miss   <- na.omit(crs$dataset[crs$test, c(crs$input, crs$target)]$failure)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL
if (length(miss.list))
{
pred <- prediction(crs$pr[-miss.list], no.miss)
} else
{
pred <- prediction(crs$pr, no.miss)
}
ROCR::plot(performance(pred, "sens", "spec"), col="#CC0000FF", lty=1, add=FALSE)
# Generate a Lift Chart for the nnet model on normalized.csv [train].
crs$pr <- predict(crs$nnet, newdata=crs$dataset[crs$test, c(crs$input, crs$target)])
#In ROCR (1.0-3) plot does not obey the add command.
# Calling the function directly (.plot.performance) does work.
.plot.performance(performance(prediction(crs$pr, crs$dataset[crs$test, c(crs$input, crs$target)]$failure),"sens", "spec"), col="#00CCCCFF", lty=2, add=TRUE)
# Add a legend to the plot.
legend("bottomleft", c("Test","Train"), col=rainbow(2, 1, .8), lty=1:2, title="nnet", inset=c(0.05, 0.05))
# Add decorations to the plot.
title(main="Sensitivity/Specificity (tpr/tnr)  normalized.csv ",
sub=paste("Rattle", format(Sys.time(), "%Y-%b-%d %H:%M:%S"), Sys.info()["user"]))
grid()
#=======================================================================
# Rattle timestamp: 2019-03-04 00:28:28 x86_64-w64-mingw32
# Score the testing dataset.
head(crs)
head(crs$dataset)
cols(crs$dataset)
colcrs$dataset)
col(crs$dataset)
length(colnames(crs$dataset))
colnames(crs$dataset)
for (i in 5:33) {
d1 <- crs$dataset[,i]
d1 <- d1[!is.na(d1)]
n <- colnames(t)[i]
show(bwplot(d1, xlab="Values", ylab="", main=n))
}
library (openxlsx)
library(lattice)
library (DataExplorer) # https://datascienceplus.com/blazing-fast-eda-in-r-with-dataexplorer/
library(corrplot)
library(dplyr)
for (i in 5:33) {
d1 <- crs$dataset[,i]
d1 <- d1[!is.na(d1)]
n <- colnames(t)[i]
show(bwplot(d1, xlab="Values", ylab="", main=n))
}
hist(crs$dataset["smart_250_raw"])
hist(crs$dataset$smart_250_raw)
hist(crs$dataset$smart_191_raw)
