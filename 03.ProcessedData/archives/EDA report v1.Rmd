---
title: 'KE 5107 CA 1: EDA '
output:
  html_document:
    df_print: paged
---

**Overview** 
We are currently using 2018 data from Back Blaze. These are SMART stats for each hard disk drive in operation and whether it fails or not. The data is mapped across days in a year.  

The Exploratory Data Analyses (EDAs) that we run on this dataset are:

1. Counts & Dimensions
  + Dimension of dataset
  + Count of dates, failed (1) vs OK (0) hard drives  
  + Proportion of failed vs OK hard drives
2. Data types 
  + Data Summary
  + Missing data & When they are missing 
  + Data with no variances 
3. Correlations between features 
4. 


This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
##### load libraries

library (openxlsx)
library(lattice)
library (DataExplorer) # https://datascienceplus.com/blazing-fast-eda-in-r-with-dataexplorer/
library(corrplot)
library(dplyr)

#### Variables 
filename <- paste("collected",".xlsx", sep = "")

#### Load dataset
hd = read.xlsx(filename, detectDates= TRUE)
```


**Dimension of Dataset**
```{r echo=TRUE}
dim(hd)
```

**Counts**

1. Unique Dates
```{r echo=TRUE}
length(unique(hd$date))
```

2. failed (1) vs OK (0) hard drives
```{r echo=TRUE}
count(hd, failure)
```

Distribution of failed (1) vs OK (0) hard drives across time?
```{r echo=TRUE}
temp <- hd %>%
  group_by(date, failure) %>%
  summarize(count=n())

xyplot(count~date, temp, groups=failure, type="l", auto.key=TRUE, main="Distribution of Failed (1) vs OK (0) Hard Drives over Time")
```



**Data types**

Summary of Dataset

```{r echo=TRUE}
summary(hd)
```

Missing Columns 
```{r}
missingHD <- colSums(is.na(hd))/nrow(hd)*100
```

Number of columns with >90% missing data: `r sum(missingHD >90)` (about `r sum(missingHD >90)/ncol(hd)*100`%)

Which columns are Missing then? 
```{r, fig.width = 12, fig.height = 20}
barchart(sort(missingHD,decreasing = TRUE), xlab = "% of missing values", main="Missing Values in Data Columns")
```

**Remove these columns:** `r colnames(hd[,missingHD >90])`


## Columns with 0 Variance 
```{r warning=FALSE}
varHD <- apply(hd, 2, var,na.rm=TRUE)
noVar <- varHD[!is.na(varHD)]

```
**Remove these columns:** `r names(noVar[noVar==0])`


``` {r}
# removing columns with > 90% missing values, 0 variance, and -Normalized variables

t <- hd[,missingHD <90]
t <- t[, -which(names(t) %in% names(noVar[noVar==0]))]
t <- t[, -grep("normalized$", colnames(t))]

```

## When would Data Go Missing?  

``` {r}
t$naVal <- rowSums(is.na(t))
xyplot(naVal~date, t, type="l", main="Volume of NAs across Time", auto.key = TRUE, xlab="Date", ylab="Count of columns with NA values")
```

The chart shows that more columns are missing between Jan to Apr. Fewer during Apr to Oct (Q2 & Q3). It might be worth using just the Apr to Oct duration for our prediction models, and filling in the missing values using a replacement technique.

## Correlation plot
```{r fig.height=12, fig.width=12}
a <- cor(t[6:32], use = "pairwise.complete.obs")
corrplot(a, method="square")
```